# 设计 - 原始上下文捕获与传输方案

## 1. 背景 (Background)
为了在前端实现“原始上下文查看器 (Raw Context Inspector)”，我们需要获取 LLM 在每一次交互中产生的**真实、原始**的输入 (Messages) 和输出 (Full Response)。

目前面临的主要挑战是：
1.  **流式输出 (Streaming Output)**: 为了保证用户体验，我们通常使用流式响应。这意味着 LLM 的输出是被打散成一个个 Token (Chunk) 的。
2.  **完整性 (Completeness)**: 调试时我们需要看的是“完整”的响应内容，而不是零散的碎片。
3.  **捕获时机 (Capture Timing)**: 必须等到流结束，才能获得完整的 Output，进而组装成一条 Log 记录。

## 2. 核心思路 (Core Concept)

### 2.1 服务端暂存 (Server-Side Buffering)
在 LangGraph 的 Node 执行层，我们将引入一个中间件或 Wrapper，用于拦截 Model 的调用。

对于 `const response = await model.invoke(messages)` 这种**非流式**调用：
- Input: `messages` (即得)
- Output: `response.content` (即得)
- Action: 直接 Log。

对于 `const stream = await model.stream(messages)` 这种**流式**调用：
- Input: `messages` (即得)
- Output: 无法立即获得。
- Action:
    1.  创建一个缓冲区 `let buffer = ""`。
    2.  遍历流 `for await (const chunk of stream)`。
    3.  实时累加: `buffer += chunk.content`。
    4.  实时 Yield chunk 给上层应用（保证用户体验）。
    5.  流结束后，`buffer` 即为完整 Output。
    6.  Log (Input + Buffered Output)。

### 2.2 数据传输 (Transmission)
一旦在服务端拿到了完整的 Input/Output Log，我们需要将其发送给前端。
由于 HTTP 响应是流式的，我们不能“回头”去修改头部。
最佳方案是利用 AI SDK 的协议或 LangGraph 的 Event 机制，将 Log 对象作为一个**特殊的 Data Chunk** 注入到流中。

## 3. 详细设计 (Detailed Design)

### 3.1 模型调用封装 (`lib/model-tracer.ts`)

为了不侵入现有业务逻辑，我们封装一个 `tracedInvoke` 或 `tracedStream` 函数。

```typescript
import { BaseChatModel } from "@langchain/core/language_models/chat_models";
import { BaseMessage } from "@langchain/core/messages";

export interface TraceLog {
  timestamp: number;
  modelId: string;
  inputs: any[]; // Serialized messages
  output: string;
}

/**
 * 封装模型调用，自动记录 Trace Log
 */
export async function tracedInvoke(
  model: BaseChatModel, 
  messages: BaseMessage[],
  config?: { runName?: string }
): Promise<BaseMessage> {
  // 1. 记录输入
  const inputs = messages.map(m => m.toDict());
  const start = Date.now();
  console.log(`[Trace Start] ${config?.runName || 'Model'}`);

  // 2. 执行原始调用 (假设此处为 blocking invoke，如果是 stream 需用另一套逻辑)
  const response = await model.invoke(messages);

  // 3. 记录输出
  const traceLog: TraceLog = {
    timestamp: start,
    modelId: model.name,
    inputs: inputs,
    output: response.content.toString()
  };

  // 4. 服务端日志 (这一步已达成用户第一阶段目标)
  console.log("=== RAW CONTEXT TRACE ===");
  console.log(JSON.stringify(traceLog, null, 2));
  console.log("=========================");

  // 5. TODO: Inject into Stream (Phase 2)
  // dispatchCustomEvent("trace_log", traceLog); 

  return response;
}
```

### 3.2 针对 Streaming 的封装 (Advanced)

如果我们需要在 Graph Node 中支持流式输出，同时捕获日志：

```typescript
export async function* tracedStream(
  model: BaseChatModel,
  messages: BaseMessage[]
) {
  const inputs = messages.map(m => m.toDict());
  let fullContent = "";
  
  const stream = await model.stream(messages);
  
  for await (const chunk of stream) {
    fullContent += chunk.content.toString();
    yield chunk; // Pass through to downstream
  }
  
  // 流结束，生成日志
  const traceLog = {
    inputs,
    output: fullContent
  };
  
  console.log("=== STREAM TRACE ===", JSON.stringify(traceLog));
  // dispatchCustomEvent("trace_log", traceLog);
}
```

### 3.3 集成到 Graph Node

修改 `assistants/general-chat/graph.ts`:

```typescript
// Old
// const response = await model.invoke(messages);

// New
const response = await tracedInvoke(model, messages, { runName: "GeneralChat" });
```

## 4. 下一步行动 (Action Items)

1.  **创建 Trace 工具**: 在 `lib/` 下实现 `tracedInvoke` (优先支持 Invoke 模式，因为当前代码使用 invoke)。
2.  **应用封装**: 将 `assistants/general-chat/graph.ts` 中的 `model.invoke` 替换为 `tracedInvoke`。
3.  **验证日志**: 运行应用，观察服务端控制台是否输出了标准的 JSON 格式日志。
4.  **传输对接 (后续)**: 在 `tracedInvoke` 内部集成 `dispatchCustomEvent`，配合前端 `ContextViewerObserver` 实现端到端可视。
